name: "[ARC][CPU]  Run Llama-models Tests"

on:
  pull_request:
  workflow_dispatch:
    inputs:
      runner:
        description: 'GHA Runner Scale Set label to run workflow on.'
        required: true
        default: llama-models-fork-gha-runnes-gpu

      debug:
        description: 'Run debugging steps?'
        required: false
        default: "true"

      sleep_time:
        description: '[DEBUG] sleep time for debugging'
        required: true
        default: "60"

      branch:
        description: "Branch parameter to control which branch to checkout"
        required: true
        default: "main"

env: 
  TOKENIZER_PATH: "models/llama3/api/tokenizer.py"  
  MODELS_PATH: '/data/llama3/models'
  DEFAULT_MODEL: 'llama-31-8b'

jobs:
  execute_workflow:
    name: Execute workload on Self-Hosted CPU k8s runner
    defaults:
      run:
        shell: bash # default shell to run all steps for a given job.
    runs-on: ${{ github.event.inputs.runner != '' && github.event.inputs.runner || 'llama-models-gha-runnes-gpu' }}
    steps:
      - name: "[DEBUG] ls -la mounted EFS volume"
        id: efs_volume
        continue-on-error: true
        if: github.event.inputs.debug == 'true'
        run: |
            echo "========= Content of the EFS mount ============="
            ls -la ${{ env.MODELS_PATH }}

      - name: "Check if selected model exists in EFS volume"
        id: check_if_model_exists
        continue-on-error: true
        run: |
            if [ ! -d ${{ env.MODELS_PATH }}/${{ inputs.model }} ]; then
                echo "Model '${{ inputs.model }}' does not exist in mounted EFS volume, Terminating workflow."
                exit 1
            fi
            echo "Content of '${{ inputs.model }}' model"
            ls -la ${{ env.MODELS_PATH }}/${{ inputs.model }}

            exit 0

      - name: "[DEBUG] Get runner container OS information"
        id: os_info
        if: ${{ github.event.inputs.debug == 'true' }}
        run: |
            cat /etc/os-release

      - name: "Checkout 'meta-llama/llama-models' repository"
        id: checkout
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.inputs.branch }}

      - name: "[DEBUG] Content of the repository after checkout"
        id: content_after_checkout
        if: ${{ github.event.inputs.debug == 'true' }}
        run: |
            ls -la ${GITHUB_WORKSPACE}

      # Place sleep step before the test execution to "exec" into the test k8s POD and run tests manually to identify what dependencies are being used.
      - name: "[DEBUG] sleep"
        id: sleep
        if: ${{ github.event.inputs.debug == 'true' && github.event.inputs.sleep_time != '' }}
        run: |
            sleep ${{ inputs.sleep_time }}

      - name: "Installing 'apt' required packages"
        id: apt_install
        run: |
          echo "[STEP] Installing 'apt' required packages"
          sudo apt update -y
          sudo apt upgrade -y
          sudo apt install python3-pip -y

      - name: "Installing 'llama-models' dependencies"
        id: pip_install
        run: |
          echo "[STEP] Installing 'llama-models' models"
          pip install -U pip setuptools
          pip install -r requirements.txt
          pip install blobfile
          pip install llama-models
          pip install xmlrunner
          pip install pytest 
          #export PYTHONPATH=$PYTHONPATH:$(pwd)

      #- name: Run Tokenizer tests
      #  id: run_tests_tokenizer
      #  if: always()
      #  run: |
      #    echo "[STEP]Running Tokenizer tests on Self-Hosted k8s ARC Runner"
      #    cd $GITHUB_WORKSPACE && python3 -m unittest models/llama3/api/test_tokenizer.py

      #- name: Run tool utils tests
      #  id: run_tests_tool_utils
      #  if: always()
      #  run: |
      #    echo "[STEP] Running tool utils test on Self-Hosted k8s ARC Runner"
      #    cd $GITHUB_WORKSPACE && python3 -m unittest models/llama3/tests/api/test_tool_utils.py --junitxml="$GITHUB_WORKSPACE/result.xml"

      - name: "Running PyTest tests on Self-Hosted k8s ARC Runner"
        id: pytest
        run: |
          echo "Running PyTest tests at 'GITHUB_WORKSPACE' path: ${GITHUB_WORKSPACE} | path: ${{ github.workspace }}"
          cd ${{ github.workspace }} && python3 -m pytest --junitxml="${{ github.workspace }}/result.xml"

      - name Test Summary
        uses: test-summary/action@v2
        with:
          paths: "${{ github.workspace }}/result.xml"
        if: always()

      #- name: Publish Test Summary
      #- name: Test Summary
      #  uses: test-summary/action@v2
      #  with:
      #    paths: "${{ github.workspace }}/result.xml"
      #    output: test-summary.md
      #  if: always()

      #- name: Upload test summary
      #  uses: actions/upload-artifact@v3
      #  with:
      #    name: test-summary
      #    path: test-summary.md
      #  if: always()
